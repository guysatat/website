<!DOCTYPE html>
<html>
<head>

    <meta charset="utf-8" />
    <title>Seeing Through Fog</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <meta property="og:title" content="Seeing Through Fog" />
    <meta property="og:type"               content="website" />
    <meta property="og:url"               content="https://www.guysatat.com/fog/index.html" />
    <meta property="og:description" content="Seeing through fog as if it wasn't there." />
    <meta property="og:image" content="https://www.guysatat.com/images/fog.jpg" />

    <link rel="stylesheet" href="assets/css/main.css" />
    <link rel="stylesheet" href="assets/css/overwrite.css" />

    <style>
    	* { padding: 0; margin: 0; }
    	canvas { background: #eee; display: block; margin: 0 auto; }
    </style>

    <link href="https://fonts.googleapis.com/css?family=Lato:100|Roboto" rel="stylesheet">
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">


	<!-- Global Site Tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-106666434-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments)};
  gtag('js', new Date());

  gtag('config', 'UA-106666434-1');
</script>

</head>
<body>

<!---
  <section id="text" class="wrapper">
  		<nav id="nav">
  			<ul class="container">
  				<li><a href="../index.html#Top">Home</a></li>
  				<li><a href="../index.html#News">News</a></li>
  				<li><a href="../index.html#Projects">Projects</a></li>
  				<li><a href="../index.html#About">About Me</a></li>
  				<li><a href="../index.html#Publications">Publications</a></li>
  				<li><a href="../index.html#Contact">Contact</a></li>
  			</ul>
  		</nav>
  </section>
  --->

  <section id="text" class="wrapper">




    <header class="major">

      <div id="header_left_col">
        <a href="http://media.mit.edu" target="_blank"><img class="mlIcons" src="MIT_ML_Logo_R_RGB1.jpg" height="100"/></a>
      </div>
      <div id="header_center">
        <h2>Towards Photography Through Realistic Fog</h2>
        <h3>Seeing through dense fog as if it wasn't there</h3>
      </div>
      <div id="header_right_col">
        <a href="http://cameraculture.media.mit.edu" target="_blank"><img class="mlIcons" src="CameraCulture_Logo_R_RGB1.jpg" height="100"/></a>
      </div>

    </header>
    <header class="major">
      <h4><a href="https://www.guysatat.com/" target="_blank">Guy Satat</a>,
        <a href="http://www.matthewtancik.com" target="_blank">Matthew Tancik</a>,
        <a href="http://media.mit.edu/~raskar/" target="_blank">Ramesh Raskar</a></h4>
    </header>


    <div class="inner">
      <section>
            <p style="margin: 0 0 0em 0">
              <b>A technique to see through dense, dynamic, and heterogeneous fog conditions. The technique, based on visible light, uses hardware that is similar to LIDAR to recover the target depth and reflectance.</b>

            </p>

            <p style="text-align: center">
              <br><a href="http://lemelson.mit.edu/winners/guy-satat" target="_blank"><img src="../images/lemelson.jpg" alt="" height="25"/> student prize for 2018.</a><br />

              <a href="https://auto-sens.com/awards-winners-2019/" target="_blank"><img src="../images/autosens.jpg" alt="" height="35" /> Most Influential Research 2019 award</a>.<br />

            </p>
<p>

  <div class="custom_im"><div class="centered_im 8u 12u$(small)">

    <div id="scaleVid"><div class="videoWrapper">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/CkR1UowJF0w?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div></div>
  </div></div>


              <div class="box alt">
                <div class="row uniform">
                  <div class="6u 12u$(small)"><span class="image fit"><img src="graphics/sketch_car.jpg" alt="" /></span></div>
                  <div class="6u 12u$(small)"><span class="image fit"><img src="graphics/sketch_drone.jpg" alt="" /></span></div>
                </div>
              </div>








            </p>

      </section>


      <section>
          <div class='section_header'>Overview</div>

          <p style="margin: 0 0 0em 0">
          The system is based on ultrafast measurements, used to computationally remove inclement weather conditions such as fog, and produce a photo and depth map as if the fog weren’t there (with contrast improved by 6.5x in dense fog conditions). The hardware is very similar to <a href="https://en.wikipedia.org/wiki/Lidar/" target="_blank">LIDAR</a> and is based on ultrafast sensing. The time-resolved measured photons are used to computationally subtract the fog from the measurement and recover the target reflectance and depth. Covered by <a href="http://news.mit.edu/2018/depth-sensing-imaging-system-can-peer-through-fog-0321" target="_blank">MIT News</a>.
        </p>
          <div class="custom_im"><div class="centered_im 8u 12u$(small)">
            <div id="scaleVid"><div class="videoWrapper">
              <iframe width="560" height="315" src="https://www.youtube.com/embed/0EU1tAkL-h4?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              </div></div>
          </div></div>

          <p>
            The measurement is based on a SPAD camera (single photon avalanche diode) that time tags individual detected photons. A pulsed visible laser is used for illumination. The suggested approach is based on a probabilistic algorithm that first estimates the fog properties (background). Then the background is subtracted from the measurement with the fog leaving the signal photons from the target which are used to recover the target reflectance and depth. <br>

          The proposed model supports a wide range of fog densities and is able to work in fog that is heterogeneous and dynamic. The fog model is estimated directly from the measurement without prior knowledge. The motivation to use the background photons is similar to our <a href="https://www.guysatat.com/API/project_all_photon.html">All Photons Imaging</a> work in which scattered light is measured and computationally used to robustly eliminate the scattering. <br>

          Other techniques to see through fog are usually based on longer wavelengths (like RF) and provide lower resolution and poor optical contrast, restricting the ability to identify road lanes and road signs. Another alternative method is based on time-gating that locks onto a small part of the unscattered signal, this results in poor signal-to-noise ratio and limits the applicability to moving platforms and high fog densities.

          <div class="custom_im">
            <div class="centered_im 8u 12u$(small)">
            <span class="image fit" style="margin: 0.5em 0 1em 0; text-align: center">
              <b>Algorithm overview</b><br>
              <img src="graphics/overview.jpg" alt>
            </span></div>
          </div>


        </p>

        <p>
        <div class='section_header'>Applications</div><ul>
            <li> Autonomous and augmented driving in challenging weather.
            <li> Airplanes and helicopters take off, landing and low level flight in dense fog conditions.
            <li> Trains traveling at normal speeds during inclement weather conditions.
        </ul>
      </p>
    </section>




    <section>
      <div class='section_header'>Fog Demonstration</div>
      The experiments are conducted in a fog chamber. We use a fog generator with a fan to create and distribute fog inside the chamber. Each experiment starts with an empty chamber (no fog). Then the fog density slowly increases as fog is being added. We end the experiment when the visibility is less than 30 cm. To quantitatively evaluate the fog level we use optical thickness. An optical thickness of zero means no fog, and higher values correspond to more fog. Below is a video that show some examples of the fog levels used along with a photo of our experiment.


<div class="box alt">
  <div class="row uniform">
      <div class="centered_im 6u 12u$(small)"><div id="scaleVid"><div class="videoWrapper">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/yUPQD7Vb4xc?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div></div></div>

      <div class="centered_im 6u 12u$(small)"><div id="scaleVid"><div class="videoWrapper">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/5sYAsVBmOvc?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div></div></div>
    </div>
</div>

      <div class="custom_im">
        <div class="centered_im 6u 12u$(small)">
          <span class="image fit" style="margin: 0.5em 0 1em 0; text-align: center">
            <b>Experimental Setup</b><br>
            <img src="graphics/setup.jpg" alt>
          </span>
        </div>
      </div>

    </section>



    <section>
    <p>
      <div class='section_header'>Results</div>
      Below we show several videos demonstrating the reconstruction of our approach and compare it to other techniques. The videos play as fog is being added to the fog chamber. On the left are other techniques including a regular camera (coupled with an independent flashlight), a SPAD camera integration (no time resolution), and time gating with the SPAD camera. On the right is our method demonstrating reflectance and depth recovery.
    </p>

    <div class="box alt">
      <div class="row uniform">
        <div class="4u 12u$(small)"><div id="scaleVid"><div class="videoWrapper">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/e9MBHzt-JPo?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div></div></div>

        <div class="4u 12u$(small)"><div id="scaleVid"><div class="videoWrapper">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/1k49q8uaOt0?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div></div></div>

        <div class="4u 12u$(small)"><div id="scaleVid"><div class="videoWrapper">
          <iframe width="560" height="315" src="https://www.youtube.com/embed/IH8qN_F888c?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div></div></div>
      </div>
    </div>

    </section>


    <section>
      <div class='section_header'>Talk at ICCP 2018</div>

    <div class="custom_im"><div class="centered_im 8u 12u$(small)">
      <div id="scaleVid"><div class="videoWrapper">
        <iframe width="560" height="315" src="https://www.youtube.com/embed/CNo0R-reNAA?rel=0&amp;start=79" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div></div>
    </div></div>
</section>


    <section>
    <p style="margin: 0 0 0em 0">
      <div class='section_header'>Technical Paper and Materials (local copy)</div>
      <ul>
      <li> <a href="materials/TowardsPhotographyThroughRealisticFog.pdf" target="_blank">Main text</a>
      <li> <a href="materials/Supplemental_TowardsPhotographyThroughRealisticFog.pdf" target="_blank">Supplement text</a>
      <li> Videos:
      <ul>
        <li> <a href="materials/Overview.mp4" target="_blank">Overview</a>
        <li> <a href="materials/Optical_Thickness_Over_Time.mp4" target="_blank">Optical thickness and visibility over time</a>
        <li> <a href="materials/Es_Over_Time.mp4" target="_blank">'Es' target - reconstruction over time</a>
        <li> <a href="materials/Mannequin_Close_Over_Time.mp4" target="_blank">Mannequin target (close) - reconstruction over time</a>
        <li> <a href="materials/Mannequin_Far_Over_Time.mp4" target="_blank">Mannequin target (far) - reconstruction over time</a>
      </ul>
      </ul>
    </p>
    </section>


    <section>
    <p style="margin: 0 0 0em 0">
      <div class='section_header'>Paper Citation</div>
      <ul>
      <li> <a href="https://ieeexplore.ieee.org/abstract/document/8368463/" target="_blank">G. Satat, M. Tancik and R. Raskar, "Towards Photography Through Realistic Fog", <i>IEEE International Conference on Computational Photography (ICCP)</i>, (2018).</a>
      <li> <a href="https://doi.org/10.1109/ICCPHOT.2018.8368463" target="_blank" style="text-decoration: none"> DOI:  10.1109/ICCPHOT.2018.8368463.</a><br>
      <li><pre>@inproceedings{satat2018towards,
title={Towards photography through realistic fog},
author={Satat, Guy and Tancik, Matthew and Raskar, Ramesh},
booktitle={Computational Photography (ICCP), 2018 IEEE International Conference on},
pages={1--10},
year={2018},
organization={IEEE}
}</pre>

      </ul>
    </p>
    </section>

    <section>
    <p style="margin: 0 0 0em 0">
      <div class='section_header'>Media Coverage</div>
      <ul>
      <!--
      <li> <a href="" target="_blank"></a>.
      --->
<li><a href="https://auto-sens.com/awards-winners-2019/" target="_blank">Winner AutoSens Most Influential Research 2019 award</a>.

      <li><a href="https://auto-sens.com/awards-finalists-2019/" target="_blank">Shortlist for AutoSens Most Influential Research Award</a>.

      <li> <a href="
https://ieeexplore.ieee.org/document/8653522" target="_blank">Cover by IEEE Signal Processing Magazine</a>

        <li> <a href="
http://news.mit.edu/2018/depth-sensing-imaging-system-can-peer-through-fog-0321" target="_blank">MIT News</a>

<li> <a href="http://www.wcvb.com/article/device-cutting-through-the-fog-helping-develop-future-technology/21242040" target="_blank">ABC WCVB</a>

<li> <a href="https://www.smithsonianmag.com/innovation/this-new-system-can-see-through-fog-far-better-than-humans-180968705/" target="_blank">Smithsonian Magazine</a>

<li> <a href="https://www.cnbc.com/2018/03/31/mit-developed-imaging-technology-that-can-see-through-fog.html" target="_blank">CNBC</a>

        <li> <a href="https://www.engadget.com/2018/03/22/self-driving-technology-fog-mit/" target="_blank">Engadget</a>

          <li> <a href="https://app.box.com/shared/static/2s6x3cj21dw7v53p3s3gm6gmutj29sav.pdf" target="_blank">Top 100 Science Spinoffs PDF Report</a>


        <li> <a href="https://gizmodo.com/mit-developed-a-way-for-cars-to-see-through-fog-when-hu-1823956815" target="_blank">Gizmodo</a>

        <li> <a href="https://www.photonics.com/a63254/Imaging_System_Navigates_Fog_Better_Than_Human" target="_blank">Photonics Media</a>

<li> <a href="http://www.ibtimes.com/laser-technology-see-through-dense-fog-while-driving-developed-mit-2665069" target="_blank">International Business Times</a>

          <li> <a href="https://phys.org/news/2018-03-depth-sensing-imaging-peer-fog.html" target="_blank">Phys.org</a>

<li> <a href="https://www.autoevolution.com/news/mit-tech-could-allow-self-driving-cars-to-see-through-fog-124781.html" target="_blank">Auto Evolution</a>

          <li> <a href="https://www.theregister.co.uk/2018/03/22/self_driving_cars_fog/" target="_blank">The Register</a>

            <li> <a href="https://www.rdmag.com/article/2018/03/new-system-helps-self-driving-cars-see-through-thick-fog" target="_blank">R&D Magazine</a>

<li> <a href="https://www.inverse.com/article/42632-new-car-visual-system-sees-through-fog" target="_blank">Inverse</a>

<!--              <li> <a href="https://www.sciencedaily.com/releases/2018/03/180321110857.htm" target="_blank">Science Daily</a>

<li> <a href="https://www.techspot.com/news/73842-mit-researchers-have-discovered-way-help-self-driving.html" target="_blank">TechSpot</a>
-->



      </ul>
    </p>
    </section>



    <section>
    <p style="margin: 0 0 0em 0">
      <div class='section_header'>Frequently Asked Questions</div>

      <b>Q: What are the main advantages of this method?</b><br>
      A: There are several key advantages:<ul>
      <li> Simplicity of the reconstruction algorithm allows it to run in real-time.
      <li> The approach doesn't assume prior knowledge about the fog density, and it works with a wide range fog densities.
      <li> The method is pixel-wise (each pixel estimates the fog properties and target independently), thus it naturally works with heterogeneous scenes and fog.
      <li> The required hardware is similar to LIDAR that is commonly used in self-driving cars.
      <li> Using visible light, it is possible to read road signs and detect lane markings.
      </ul>

      <b>Q: What are some applications?</b><br>
      A: Augmenting a human driver and enabling self-driving cars to operate in challenging weather; allowing drones to navigate and follow targets in inclement weather; improving flight safety of airplanes and helicopters during takeoffs, landings and low-level flights in extreme weather; and allowing trains to travel faster in low visibility.
      <br><br>

      <b>Q: What do you mean by Realistic fog?</b><br>
      A: Our experiments are conducted with a water-based fog generator combined with a fan. This results in fog with variable densities (no fog to very dense), that is moving (dynamic) and heterogeneous (patchy).
      <br><br>


      <b>Q: Why is visible light essential for imaging through fog (why not just RADAR)?</b><br>
      A: Imaging in the visible part of the electromagnetic spectrum provides good resolution (short wavelength compared to RF), and good optical contrast (different materials appear very different under visible light). The latter is key to identifying road lane markings and reading road signs.
      <br><br>

      <b>Q: What is a SPAD camera?</b><br>
      A: SPAD (single photon avalanche diode) camera time tags individual photons as they are detected. Each pixel records one photon per laser pulse (our laser emits millions of pulses per second), and our method requires only a few tens of thousands photons.
      <br><br>

      <b>Q: Is this a probabilistic framework? What does that mean?</b><br>
      A: We develop a probabilistic framework to model scattered photons from fog. This model is estimated from the raw measurement and is used to answer the question "what is the probability that a photon was scattered from fog or target?".
      <br><br>

      <b>Q: How does the SPAD single photon sensitivity help?</b><br>
      A: The single photon sensitivity helps us in the development of a probabilistic framework. Effectively for each detected photon we ask what is the probability that this photon reflected from the fog or target. Perhaps more importantly, the SPAD measurement noise is much better (less noise) when compared to traditional cameras.
      <br><br>

      <b>Q: How does the time-resolved sensing help?</b><br>
      A: We want to know what is the probability that a photon was reflected from the fog or target. A more robust way to answer that is asking given the prior knowledge that the photon was measured at a specific time, what is the probability that it was reflected from the fog or target? Intuitively, more information (time in this case) helps making better estimates. In the paper we specifically show that the probabilities to measure a photon from target and fog are different as a function of time.
      <br><br>

      <b>Q: What is optical thickness?</b><br>
      A: Optical thickness is a measure to level of scattering. If we want to measure optical thickness of fog at a given time point, we measure the light intensity at that time, and compare it the light intensity without fog. With higher fog densities, and light intensity drops due to the scattering. Optical thickness is a dimensionless quantity so it's easy to compare it to different experiments. See more <a href="https://en.wikipedia.org/wiki/Optical_depth" target="_blank">here</a>.
      <br><br>

      <b>Q: What are the main limitations of this method?</b><br>
      A: Because the approach is pixel wise (each pixel operates independently) it ignores spatial blur that is induced by the fog. While this effect was very minor in our measurements it is possible that it would become more apparent in other scenarios. <br>
      Another limitation is the acquisition time. While we can generate a new result with every new detected photon (100 microseconds), we rely on recently detected photons. In our setup we used a history of 2 seconds, thus this limits the immediate application for a moving platform. We hope that stronger laser and better hardware would improve this.
      <br><br>

    </p>
    </section>


    <section>
      <div class="custom_im">
        <div class="centered_im 4u 12u$(small)">
        <span class="image fit" style="margin: 0.5em 0 1em 0; text-align: center">
          <img src="graphics/fog_bath.jpg" alt>
        </span></div>
      </div>
    </section>


    <section>
    <p style="margin: 0 0 0em 0">
      <div class='section_header'>Camera Culture Related Works :</div><ul>

     <li> <a href="../calib_inv/index.html" target="_blank"  style="text-decoration: none">
        G. Satat, M. Tancik, O. Gupta, B. Heshmat and R. Raskar, "Object Classification through Scattering Media with Deep Learning on Time Resolved Measurement," <i>Optics Express</i> Vol. 25, 17466-17479 (2017).
      </a>

      <li> <a href="https://www.guysatat.com/singlepixel" target="_blank" style="text-decoration: none"> G. Satat, M. Tancik and R. Raskar, "Lensless Imaging with Compressive Ultrafast Sensing", <i>IEEE Trans. Computational Imaging</i>, (2017).</a>


        <li> <a href="http://web.media.mit.edu/~achoo/beat/index.html" target="_blank" style="text-decoration: none">A. Kadambi and R. Raskar,  “Rethinking Machine Vision Time of Flight With GHz Heterodyning”, <i>IEEE Access</i>, Vol. 5, (2017).</a>

      <li> <a href="https://www.guysatat.com/API/project_all_photon.html" target="_blank" style="text-decoration: none">G. Satat, B. Heshmat, D. Raviv and R. Raskar,  “All Photons Imaging Through Volumetric Scattering”, <i>Nature Scientific Reports</i>, Vol. 6, 33946, (2016).</a>



      <li> <a href="http://web.media.mit.edu/~barmak/Brush%20Project.html" target="_blank" style="text-decoration: none">B. Heshmat, I. H. Lee, R. Raskar, "Optical brush: Imaging through permuted probes," Nature Scientific Reports, Vol. 6, 20217, (2016).</a>

      <li> <a href="http://web.mit.edu/naik/www/assets/pdf/satat_etal_ultrafast_survey_16.pdf" target="_blank" style="text-decoration: none">G. Satat, B. Heshmat, N. Naik, A. R. Sanchez and R. Raskar, "Advances in ultrafast optics and imaging applications," in <i>SPIE 2016</i> (invited).</a>

      <li> <a href="https://www.guysatat.com/fl/Fluorescent/Locating%20and%20classifying%20fluorescent%20tags%20behind%20turbid%20layers%20using%20time-resolved%20inversion.pdf" target="_blank" style="text-decoration: none">G. Satat, B. Heshmat, C. Barsi, D. Raviv, O. Chen, M.G. Bawendi and R. Raskar,  “Locating and Classifying Fluorescent Tags Behind Turbid Layers Non-Invasively Using Sparsity-Based Time-Resolved Inversion,” <i>Nature Communications</i>, Vol. 6, 6796 (2015).</a>

      <li> <a href="http://extremelight.eps.hw.ac.uk/publications/NC-photon-flight-Gariepy(2015).pdf" target="_blank" style="text-decoration: none">G. Gariepy, et al., "Single-photon sensitive light-in-fight imaging," <i>Nature Communications</i>, Vol. 6, 6021 (2015).</a>

      <li> <a href="https://www.osapublishing.org/abstract.cfm?uri=cleo_si-2014-STu3E.7" target="_blank" style="text-decoration: none">B. Heshmat, G. Satat, C. Barsi, and R. Raskar, "Single-Shot Ultrafast Imaging Using Parallax-Free Alignment with a Tilted Lenslet Array," in <i>CLEO: 2014</i> (oral).</a>

     <li> <a href="http://web.media.mit.edu/~raskar/femto/papers/3d/3DShapeAroundCornerRaskar2011.pdf" target="_blank" style="text-decoration: none">A. Velten, et al., "Recovering three-dimensional shape around a corner using ultrafast time-of-flight imaging," <i>Nature communications</i>, Vol. 3, 745 (2012).</a>

      <li> <a href="https://www.researchgate.net/profile/Andreas_Velten/publication/220721073_Slow_art_with_a_trillion_frames_per_second_camera/links/0912f50f02fb790150000000.pdf" target="_blank" style="text-decoration: none">A. Velten, et al., "Slow art with a trillion frames per second camera," <i>ACM SIGGRAPH 2011 Talks</i> ACM, (2011).</a>

      <li> <a href="https://dspace.mit.edu/handle/1721.1/67888" target="_blank" style="text-decoration: none">R. Raskar, and D. James, "5d time-light transport matrix: What can we reason about scene properties," <i>Int. Memo</i> 7 (2008).</a>

      </ul>

    </p>
    </section>




    <section class="wrapper">
      <div class='section_header'>Camera Culture Related Talks</div>

        <div class="row uniform">
          <div class="6u 12u$(small)"><span class="vid_thumb image fit">
            <a href="https://vimeo.com/520293400/288e96dd41" target="_blank">
            Seeing Through Fog for Autonomous Vehicles
            <img src="autosens2.jpg" alt="" />
            </a>
          </span></div>

          <div class="6u 12u$(small)"><span class="vid_thumb image fit">
            <a href="https://vimeo.com/520894388/0bd1336c26" target="_blank">
            Imaging for Autonomous Vehicles
            <img src="autosens.jpg" alt="" />
            </a>
          </span></div>

          <div class="6u 12u$(small)"><span class="vid_thumb image fit">
            <a href="https://www.youtube.com/watch?v=IJ2cMqQOOHM" target="_blank">
            Seeing Through Scattering
            <img src="xrayvision.jpg" alt="" />
            </a>
          </span></div>

          <div class="6u 12u$(small)"><span class="vid_thumb image fit">
            <a href="https://www.youtube.com/watch?v=3h7gq-xuXLA" target="_blank">
            The Future of Imaging
            <img src="future_of_imaging.png" alt="" />
            </a>
          </span></div>

          <div class="6u 12u$(small)"><span class="vid_thumb image fit">
            <a href="http://www.ted.com/talks/ramesh_raskar_a_camera_that_takes_one_trillion_frames_per_second?language=en" target="_blank">
            Imaging at Trillion Frames per Second
            <img src="imaging-at-trillion-frames-per-second.jpg" alt="" />
            </a>
          </span></div>

        </div>
    </section>






    </div>














<footer id="footer">
  <p class="copyright">&copy; Guy Satat 2018. All rights reserved.  <br />   HTML template: <a href="http://html5up.net">HTML5 UP</a></p>
</footer>

<!-- Scripts -->
<script src="assets/js/jquery.min.js"></script>
<script src="assets/js/jquery.scrolly.min.js"></script>
<script src="assets/js/skel.min.js"></script>
<script src="assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="assets/js/main.js"></script>


</body>
</html>
